# ğŸ§  LLM Exercise â€“ Python Tutor Chatbot

This project is an educational chatbot application powered by RAG (Retrieval-Augmented Generation), combining:

* **LLM via Groq (`gemma2-9b-it`)**
* **Embeddings from HuggingFace (`all-MiniLM-L6-v2`)**
* **Streamlit** as an alternative interface (legacy)
* **Markdown files** as the knowledge base

---

## ğŸ—‚ Project Structure

```bash
llm-exercise/
â”‚
â”œâ”€â”€ lessons/                   # Raw markdown lesson files
â”‚   â”œâ”€â”€ python_variables.md
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ lessons_faiss/             # FAISS indexes generated from lessons
â”‚   â”œâ”€â”€ python_variables/
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ backend/                   # FastAPI backend source
â”‚   â”œâ”€â”€ llm_groq.py            # RAG chain logic
â”‚   â”œâ”€â”€ build_faiss_index.py   # Indexes markdown â†’ FAISS
â”‚
â”œâ”€â”€ legacy_streamlit.py        # Streamlit-based fallback UI
â”œâ”€â”€ .env                       # Environment variables
â”œâ”€â”€ requirements.txt           # Python dependencies
```

---

## âš™ï¸ How It Works

1. **Preprocessing:**

   * Run `build_faiss_index.py` to embed markdown files into FAISS index using `all-MiniLM-L6-v2`.

2. **Backend:**

   * The `/chat` FastAPI endpoint receives `prompt` and `lesson` â†’ builds a contextual RAG chain via `llm_groq.py`.

3. **RAG Chain:**

   * Retrieves relevant chunks via FAISS
   * Combines them with user prompt
   * Uses `ChatGroq` (Groq API) to generate answers in Bahasa Indonesia.

4. **Frontend:**

   * Streamlit UI (`legacy_streamlit.py`) provides a side-by-side learning/chat interface.

---

## ğŸ“˜ Sample Lesson Files

```bash
lessons/
â”œâ”€â”€ python_variables.md
â”œâ”€â”€ python_list.md
â”œâ”€â”€ if_else.md
â”œâ”€â”€ while.md
â”œâ”€â”€ for_loops.md
â””â”€â”€ functions.md
```

---

## ğŸš€ How to Run

### 1. Generate FAISS Index

```bash
python backend/build_faiss_index.py
```

### 2. Start Frontend (Streamlit)

```bash
streamlit run legacy_streamlit.py
```

---

## ğŸ” .env File

Your `.env` file should look like:

```
GROQ_API_KEY=your_groq_api_key_here
HF_TOKEN=your_huggingface_api_key_here
```

---

## ğŸ“¦ Python Requirements

All packages are listed in `requirements.txt`. Some highlights include:

* `langchain`
* `langchain-groq`
* `langchain-ollama`
* `faiss-cpu`
* `streamlit`
* `ollama`
* `python-dotenv`

---
## âš ï¸ Important Notes

* Embedding model must be consistent across indexing and querying (`all-MiniLM-L6-v2`)
* Ensure `lesson` sent from frontend matches the folder name inside `lessons_faiss`
* If you want to use Ollama directly for LLM (instead of Groq), you can swap out the chain logic accordingly
